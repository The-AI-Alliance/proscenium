{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Literature\n",
        "\n",
        "This notebook demonstrates:\n",
        "\n",
        "1. Downloading two books from Project Gutenberg\n",
        "2. Chunking them\n",
        "3. Storing in a vector database\n",
        "4. Usint the query to find a similar chunk in the vector database to form the context for an LLM call (Retrieval Augmented Generation, aka \"RAG\")\n"
      ],
      "metadata": {
        "id": "01POz8PiyB5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1FxfLnYmRY7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/The-AI-Alliance/proscenium.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd proscenium"
      ],
      "metadata": {
        "id": "ggeRbYZ9mSsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install ."
      ],
      "metadata": {
        "collapsed": true,
        "id": "u_UwY3bfmSqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('TOGETHER_API_KEY')\n",
        "os.environ['TOGETHER_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "nhoa_GJWmSnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich import print\n",
        "from rich.panel import Panel\n",
        "from rich.prompt import Prompt\n",
        "\n",
        "import asyncio\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
      ],
      "metadata": {
        "id": "S_iC1uYgmSk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"together:meta-llama/Llama-3-70b-chat-hf\"\n",
        "\n",
        "print(model_id)"
      ],
      "metadata": {
        "id": "xDt2b7FEmSiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Vector Database from Document Chunks"
      ],
      "metadata": {
        "id": "LQIG0H2Ip8x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import demo.domains.literature as domain\n",
        "from proscenium.verbs.read import url_to_file\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "for book in domain.books:\n",
        "    print(\"Book:\", book.title)\n",
        "    asyncio.run(url_to_file(book.url, book.data_file))\n",
        "    print(\"Local copy to chunk:\", book.data_file)\n"
      ],
      "metadata": {
        "id": "QmT-3-IvmSfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from proscenium.verbs.vector_database import embedding_function\n",
        "\n",
        "embedding_fn = embedding_function(domain.embedding_model_id)\n",
        "print(\"Embedding model\", domain.embedding_model_id)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5_OCVu_NmScy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from proscenium.verbs.vector_database import vector_db\n",
        "\n",
        "milvus_uri = \"file:/milvus.db\"\n",
        "\n",
        "vector_db_client = vector_db(milvus_uri, overwrite=True)\n",
        "print(\"Vector db at uri\", milvus_uri)"
      ],
      "metadata": {
        "id": "XBbGDyi_mSaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from proscenium.scripts.chunk_space import build_vector_db\n",
        "\n",
        "collection_name = \"literature_chunks\"\n",
        "\n",
        "build_vector_db([book.data_file for book in domain.books], vector_db_client, embedding_fn, collection_name)\n",
        "\n",
        "# vector_db_client.close()"
      ],
      "metadata": {
        "id": "JDPrapyZmSXU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer User Question"
      ],
      "metadata": {
        "id": "H_RgYK_ZskDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What did Hermes say to Prometheus about giving fire to humans?\""
      ],
      "metadata": {
        "id": "VxuBMD3rsjoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from proscenium.scripts.rag import answer_question\n",
        "\n",
        "answer = answer_question(\n",
        "    question, domain.model_id, vector_db_client, embedding_fn, collection_name, True\n",
        ")\n",
        "\n",
        "print(Panel(answer, title=\"Assistant\"))\n"
      ],
      "metadata": {
        "id": "4DcRYOR2mSQz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}